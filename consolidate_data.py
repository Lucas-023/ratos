# consolidate_data.py - Consolida√ß√£o de Dados de Treino/Valida√ß√£o com Normaliza√ß√£o Z-Score
# VERS√ÉO DE ESTABILIDADE M√ÅXIMA PARA RESOLVER O NAN

import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import List, Any
import os 

# =========================================================
# CONFIGURA√á√ïES E PAR√ÇMETROS
# =========================================================

# üö® AJUSTE AQUI: Caminho para os arquivos Parquet *PROCESSADOS* de TREINO üö®
BASE_PATH_TRAIN = Path("MABe-mouse-behavior-detection/processed_videos_final_fixed") 

OUTPUT_X = "consolidated_X.npy"    # Features de TREINO/VAL (NORMALIZADAS)
OUTPUT_Y = "consolidated_Y.csv"    # Labels de TREINO/VAL
X_MEAN_PATH = "X_mean.npy"         # M√©dia das Features
X_STD_PATH = "X_std.npy"           # Desvio Padr√£o das Features

# --- DEFINI√á√ïES DE COLUNAS ---
try:
    from dataloader import FEATURE_COLUMNS, TARGET_COLUMN
except ImportError:
    print("‚ö†Ô∏è Aviso: dataloader.py n√£o encontrado. Usando defini√ß√µes mockadas.")
    FEATURE_COLUMNS = [f'x_{i}' for i in range(1, 118)]
    TARGET_COLUMN = 'behavior'

N_FEATURES = len(FEATURE_COLUMNS)
parquet_files = list(BASE_PATH_TRAIN.rglob("*.parquet"))

if not parquet_files:
    print(f"‚ùå NENHUM arquivo Parquet encontrado em {BASE_PATH_TRAIN.resolve()}.")
    exit()

# =========================================================
# FUN√á√ÉO AUXILIAR: Extra√ß√£o Segura de Labels (MESMA DO ORIGINAL)
# =========================================================

def safe_extract_labels(label_raw: Any) -> List[str]:
    """
    Trata formatos de labels (incluindo arrays, NaN, e strings 'nan') 
    e retorna uma lista de strings de labels v√°lidas.
    """
    if isinstance(label_raw, (list, np.ndarray, pd.Series)):
         return [str(l).strip() for l in label_raw if str(l).strip()]
    if pd.isna(label_raw): 
        return []
    label_str = str(label_raw).strip()
    if not label_str or label_str.lower() in ('nan', '0.0', '0'):
        return []
    return [l.strip() for l in label_str.split(';') if l.strip()]


# =========================================================
# 1. PR√â-C√ÅLCULO DO TAMANHO TOTAL (PASSAGEM 0)
# =========================================================

total_frames = 0
print("üîç Calculando o n√∫mero total de frames em todos os arquivos...")
for file_path in tqdm(parquet_files, desc="Contando Frames"):
    try:
        # L√™ apenas uma coluna para contar as linhas
        df_temp = pd.read_parquet(file_path, engine='fastparquet', columns=[TARGET_COLUMN])
        total_frames += df_temp.shape[0]
    except Exception as e:
        print(f"\n‚ö†Ô∏è ERRO ao contar frames em {file_path.name}: {e}. Pulando.")

if total_frames == 0:
    print("Nenhum frame v√°lido encontrado para consolida√ß√£o. Saindo.")
    exit()

print(f"‚úÖ Total de frames a serem consolidados: {total_frames}")


# =========================================================
# 2. C√ÅLCULO DE ESTAT√çSTICAS (PRIMEIRA PASSAGEM)
# =========================================================

# Usamos float64 para garantir a precis√£o no c√°lculo da m√©dia/vari√¢ncia
sum_x = np.zeros(N_FEATURES, dtype=np.float64)
sum_x_sq = np.zeros(N_FEATURES, dtype=np.float64)
total_frames_count = 0 

print("\nüìä Iniciando a PRIMEIRA PASSAGEM: C√°lculo de M√©dia e Vari√¢ncia...")
for file_path in tqdm(parquet_files, desc="Passagem 1/2: Calculando Estat√≠sticas"):
    try:
        df = pd.read_parquet(file_path, engine='fastparquet')
        
        features_df = df.reindex(columns=FEATURE_COLUMNS, fill_value=0.0)
        
        # Converte para NumPy (float64 para precis√£o do c√°lculo)
        features_np = features_df.values.astype(np.float64) 
        
        sum_x += np.sum(features_np, axis=0)
        sum_x_sq += np.sum(features_np**2, axis=0)
        total_frames_count += len(features_np)
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è ERRO no c√°lculo de estat√≠sticas em {file_path.name}: {e}. Pulando.")
        continue

# --- C√°lculo Final do Z-Score ---
X_mean = sum_x / total_frames_count
X_var = (sum_x_sq / total_frames_count) - (X_mean**2)
X_std = np.sqrt(X_var)

# üö® CORRE√á√ÉO CR√çTICA DE ESTABILIDADE üö®
# Adiciona um epsilon de 1e-8. ISSO IMPEDE DIVIS√ÉO POR ZERO (STD=0)
# que causa Infinitos ou NaN no consolidated_X.npy
EPSILON = 1e-8
X_std[X_std < EPSILON] = EPSILON 
print(f"   STD M√çNIMO (Ap√≥s corre√ß√£o): {np.min(X_std):.1e}")

# Salva a M√©dia e o Desvio Padr√£o
np.save(X_MEAN_PATH, X_mean.astype(np.float32))
np.save(X_STD_PATH, X_std.astype(np.float32))

print(f"‚úÖ Estat√≠sticas calculadas e salvas em {X_MEAN_PATH} e {X_STD_PATH}.")


# =========================================================
# 3. CONSOLIDA√á√ÉO, NORMALIZA√á√ÉO E ESCRITA (SEGUNDA PASSAGEM)
# =========================================================

print(f"\nPr√©-alocando {OUTPUT_X} ({total_frames} linhas x {N_FEATURES} features)...")

# Pr√©-aloca o arquivo memmap para escrever os dados NORMALIZADOS
X_memmap = np.memmap(
    OUTPUT_X, 
    dtype=np.float32, 
    mode='w+', 
    shape=(total_frames, N_FEATURES)
)

# Reseta o arquivo CSV de labels e escreve o cabe√ßalho
with open(OUTPUT_Y, 'w', encoding='utf-8') as f_csv:
    f_csv.write(f"{TARGET_COLUMN}\n")

current_index = 0
print("üöÄ Iniciando a SEGUNDA PASSAGEM: Normaliza√ß√£o e Grava√ß√£o...")

for file_path in tqdm(parquet_files, desc="Passagem 2/2: Gravando Normalizado"):
    try:
        df = pd.read_parquet(file_path, engine='fastparquet')
        n_rows = len(df)
        
        # --- A) Grava√ß√£o das FEATURES (X) ---
        features_df = df.reindex(columns=FEATURE_COLUMNS, fill_value=0.0)
        features_np = features_df.values.astype(np.float32)
        
        # NORMALIZA√á√ÉO Z-SCORE (AGORA EST√ÅVEL)
        features_normalized = (features_np - X_mean.astype(np.float32)) / X_std.astype(np.float32)
        
        # Escreve o bloco de dados NORMALIZADO no memmap
        X_memmap[current_index : current_index + n_rows] = features_normalized
        
        # --- B) Grava√ß√£o dos LABELS (Y) ---
        labels_series = df[TARGET_COLUMN].apply(
            lambda x: ";".join(safe_extract_labels(x))
        )
        
        labels_series.to_csv(
            OUTPUT_Y, 
            mode='a', 
            index=False, 
            header=False,
            encoding='utf-8'
        )
        
        current_index += n_rows
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è ERRO (PULANDO ARQUIVO) ao normalizar e gravar {file_path.name}: {e}. A consolida√ß√£o continua.")
        continue 

# Garante que todos os dados tenham sido escritos no disco
X_memmap.flush() 

print("\n----------------------------------------------------")
print("‚úÖ CONSOLIDA√á√ÉO DE TREINO/VAL CONCLU√çDA E NORMALIZADA!")
print("Os arquivos NPY devem estar est√°veis agora.")
print("----------------------------------------------------")