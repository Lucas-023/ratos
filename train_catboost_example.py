from __future__ import annotations

"""
Treinamento CatBoost com uso m√≠nimo de RAM.

Fluxo:
1. Usa arrays memmap (on-disk) para n√£o carregar tudo em RAM.
2. Faz o merge das features num√©ricas + categ√≥ricas em blocos.
3. Constr√≥i a matriz multi-hot de labels tamb√©m em memmap.
4. Treina esquema One-vs-Rest reutilizando os mesmos arquivos mapeados.
"""

import argparse
import warnings
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import pickle
import pyarrow.parquet as pq
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, hamming_loss

warnings.filterwarnings("ignore")

# =================================================================
# CONFIGURA√á√ïES
# =================================================================

X_PATH = Path("consolidated_X_catboost.npy")
Y_PATH = Path("consolidated_Y_catboost.csv")
CATEGORICAL_PATH = Path("consolidated_X_catboost_categorical.parquet")
CATEGORICAL_INFO_PATH = Path("categorical_info_catboost.pkl")

# Caches on-disk (podem ser reutilizados entre execu√ß√µes)
COMBINED_X_CACHE = Path("consolidated_X_catboost_with_cats.npy")
Y_MULTI_HOT_CACHE = Path("consolidated_Y_multi_hot.npy")

# Tamanho de bloco para processar dados sem estourar RAM
CHUNK_ROWS = 50_000


# =================================================================
# FUN√á√ïES AUXILIARES
# =================================================================

def parse_multi_label(label_str: str | Any) -> list[str]:
    """Converte string (com ';') em lista de labels."""
    # Trata valores NaN/None
    if pd.isna(label_str) or label_str is None:
        return []
    
    # Converte para string
    label_str = str(label_str)
    
    # Trata strings vazias
    if label_str.strip() == "" or label_str.lower() in ("nan", "none", "null"):
        return []
    
    # Se j√° √© uma lista (pode acontecer se o CSV foi lido incorretamente)
    if isinstance(label_str, (list, tuple)):
        return [str(l).strip() for l in label_str if str(l).strip() and str(l).lower() not in ("nan", "none", "null")]
    
    # Divide por ';' e limpa
    labels = [lbl.strip() for lbl in label_str.split(";") if lbl.strip() and lbl.lower() not in ("nan", "none", "null")]
    return labels


def load_label_space(y_df: pd.DataFrame) -> tuple[list[str], dict[str, int]]:
    """Cria lista ordenada de labels e mapa label->indice."""
    all_labels = set()
    total_rows = len(y_df)
    non_empty_count = 0
    
    print(f"   üîç Analisando {total_rows:,} registros de labels...")
    
    for idx, label_str in enumerate(y_df["behavior"]):
        parsed = parse_multi_label(label_str)
        if parsed:
            non_empty_count += 1
            all_labels.update(parsed)
        
        # Mostra exemplos dos primeiros registros
        if idx < 5:
            print(f"      Exemplo {idx+1}: tipo={type(label_str)}, valor={repr(label_str)[:100]}")
            print(f"         ‚Üí Parseado: {parsed}")

    print(f"   ‚Ä¢ Registros com labels n√£o vazios: {non_empty_count:,} de {total_rows:,}")
    print(f"   ‚Ä¢ Labels √∫nicos encontrados: {len(all_labels)}")
    
    if len(all_labels) == 0:
        print("   ‚ö†Ô∏è NENHUM label encontrado! Verificando tipos de dados...")
        # Diagn√≥stico adicional
        sample_values = y_df["behavior"].head(20).tolist()
        print(f"   ‚Ä¢ Primeiros 20 valores brutos:")
        for i, val in enumerate(sample_values):
            print(f"      [{i}] tipo={type(val)}, valor={repr(val)[:150]}")
    
    ordered_labels = sorted(all_labels)
    label_to_idx = {label: idx for idx, label in enumerate(ordered_labels)}
    
    if len(ordered_labels) > 0:
        print(f"   ‚Ä¢ Primeiros 10 labels: {ordered_labels[:10]}")
    
    return ordered_labels, label_to_idx


def ensure_multi_hot_cache(
    y_df: pd.DataFrame,
    label_to_idx: dict[str, int],
    rebuild: bool = False,
) -> np.memmap:
    """Gera/abre o cache memmap das labels multi-hot."""
    n_samples = len(y_df)
    n_classes = len(label_to_idx)

    if Y_MULTI_HOT_CACHE.exists() and not rebuild:
        print(f"üìÇ Reutilizando cache de labels multi-hot: {Y_MULTI_HOT_CACHE}")
        cached = np.load(Y_MULTI_HOT_CACHE, mmap_mode="r+")
        # Verifica se o cache tem o tamanho correto
        if cached.shape != (n_samples, n_classes):
            print(f"   ‚ö†Ô∏è Cache tem tamanho incorreto {cached.shape}, esperado ({n_samples}, {n_classes}). Reconstruindo...")
            rebuild = True
        else:
            return cached

    if rebuild or not Y_MULTI_HOT_CACHE.exists():
        print("üß± Construindo cache multi-hot em disco...")
        y_multi_hot = np.lib.format.open_memmap(
            Y_MULTI_HOT_CACHE,
            mode="w+",
            dtype=np.float32,
            shape=(n_samples, n_classes),
        )
        y_multi_hot[:] = 0.0

        labels_processed = 0
        labels_not_found = set()
        labels_found = set()
        
        for idx, label_str in enumerate(y_df["behavior"]):
            parsed_labels = parse_multi_label(label_str)
            for label in parsed_labels:
                if label in label_to_idx:
                    y_multi_hot[idx, label_to_idx[label]] = 1.0
                    labels_processed += 1
                    labels_found.add(label)
                else:
                    labels_not_found.add(label)
                    # Mostra os primeiros labels n√£o encontrados para diagn√≥stico
                    if len(labels_not_found) <= 10:
                        print(f"      ‚ö†Ô∏è Label n√£o encontrado no mapeamento: {repr(label)}")

            if (idx + 1) % 100_000 == 0:
                print(f"   ‚Ä¢ Processados {idx+1:,} registros de labels ({labels_processed:,} labels ativos)...")
        
        # Diagn√≥stico final
        print(f"\n   üìä Diagn√≥stico da constru√ß√£o:")
        print(f"      ‚Ä¢ Labels encontrados no mapeamento: {len(labels_found)}")
        print(f"      ‚Ä¢ Labels N√ÉO encontrados: {len(labels_not_found)}")
        if labels_not_found:
            print(f"      ‚Ä¢ Primeiros labels n√£o encontrados: {list(labels_not_found)[:10]}")
        if labels_found:
            print(f"      ‚Ä¢ Primeiros labels encontrados: {list(labels_found)[:10]}")

        y_multi_hot.flush()
        print(f"‚úÖ Cache de labels salvo em {Y_MULTI_HOT_CACHE}")
        print(f"   ‚Ä¢ Total de labels processados: {labels_processed:,}")
        
        # Verifica√ß√£o final
        total_check = np.asarray(y_multi_hot).sum()
        print(f"   ‚Ä¢ Verifica√ß√£o: {int(total_check):,} labels ativos na matriz")
        
        if total_check == 0:
            print("\n   ‚ö†Ô∏è ATEN√á√ÉO: Matriz multi-hot est√° vazia!")
            print("   Poss√≠veis causas:")
            print("   1. Labels no CSV n√£o correspondem aos labels no label_to_idx")
            print("   2. Formato dos labels no CSV est√° incorreto")
            print("   3. Problema na fun√ß√£o parse_multi_label")
        
    return np.load(Y_MULTI_HOT_CACHE, mmap_mode="r+")


def _build_cat_value_maps(categorical_info: dict) -> dict[str, dict[str, int]]:
    """Mapeia categoria -> √≠ndice inteiro para cada coluna."""
    value_maps: dict[str, dict[str, int]] = {}
    for col, info in categorical_info.items():
        categories = info.get("categories", [])
        value_maps[col] = {str(val): idx for idx, val in enumerate(categories)}
    return value_maps


def ensure_feature_cache(
    rebuild: bool,
    categorical_info: dict,
) -> tuple[np.memmap, list[int]]:
    """
    Gera/abre matriz de features (num√©ricas + categ√≥ricas codificadas) em memmap.
    O merge ocorre em blocos para mimetizar o comportamento do LazyFrameDataset.
    """
    X_numeric = np.load(X_PATH, mmap_mode="r")
    n_samples, n_numeric = X_numeric.shape

    parquet_file = pq.ParquetFile(CATEGORICAL_PATH)
    cat_columns = parquet_file.schema.names
    n_cat = len(cat_columns)
    total_features = n_numeric + n_cat

    cat_value_maps = _build_cat_value_maps(categorical_info)

    if COMBINED_X_CACHE.exists() and not rebuild:
        print(f"üìÇ Reutilizando cache de features: {COMBINED_X_CACHE}")
        combined = np.load(COMBINED_X_CACHE, mmap_mode="r+")
        cat_feature_indices = list(range(n_numeric, total_features))
        return combined, cat_feature_indices

    print("üß± Construindo cache num√©rico + categ√≥rico em disco...")
    combined = np.lib.format.open_memmap(
        COMBINED_X_CACHE,
        mode="w+",
        dtype=np.float32,
        shape=(n_samples, total_features),
    )

    # Copia num√©ricas em blocos (evita c√≥pia √∫nica gigantesca)
    for start in range(0, n_samples, CHUNK_ROWS):
        end = min(start + CHUNK_ROWS, n_samples)
        combined[start:end, :n_numeric] = X_numeric[start:end]
        if (start // CHUNK_ROWS) % 10 == 0:
            print(f"   ‚Ä¢ Copiando features num√©ricas: {end:,}/{n_samples:,}")

    # Codifica categ√≥ricas bloco a bloco
    row_start = 0
    for batch in parquet_file.iter_batches(batch_size=CHUNK_ROWS):
        batch_df = batch.to_pandas()
        batch_rows = len(batch_df)

        for col_idx, col in enumerate(cat_columns):
            mapping = cat_value_maps.get(col, {})
            unknown_idx = len(mapping)

            column_series = batch_df[col].astype("string").fillna("__unk__")
            encoded = column_series.map(
                lambda v: mapping.get(str(v), unknown_idx),
                na_action="ignore",
            ).astype(np.float32)
            combined[row_start : row_start + batch_rows, n_numeric + col_idx] = encoded.values

        row_start += batch_rows
        print(f"   ‚Ä¢ Codificando categ√≥ricas: {row_start:,}/{n_samples:,}")

    combined.flush()
    cat_feature_indices = list(range(n_numeric, total_features))
    print(f"‚úÖ Cache completo salvo em {COMBINED_X_CACHE}")
    return combined, cat_feature_indices


def train_models(
    X_train: np.ndarray,
    X_val: np.ndarray,
    y_train: np.ndarray,
    y_val: np.ndarray,
    cat_feature_indices: list[int],
    all_labels: list[str],
    iterations: int = 500,
    depth: int = 6,
    learning_rate: float = 0.1,
) -> list[CatBoostClassifier | None]:
    """Treina um modelo CatBoost One-vs-Rest por classe."""
    n_classes = y_train.shape[1]
    models: list[CatBoostClassifier | None] = []

    # Diagn√≥stico: verifica quantas classes t√™m exemplos positivos
    print("\nüîç Diagn√≥stico de classes...")
    classes_with_samples = []
    for class_idx in range(n_classes):
        y_class = np.asarray(y_train[:, class_idx])
        positive_count = int(y_class.sum())
        if positive_count > 0:
            classes_with_samples.append((class_idx, positive_count, all_labels[class_idx]))
    
    print(f"   ‚Ä¢ Total de classes: {n_classes}")
    print(f"   ‚Ä¢ Classes com exemplos positivos: {len(classes_with_samples)}")
    if len(classes_with_samples) == 0:
        print("   ‚ö†Ô∏è NENHUMA classe tem exemplos positivos! Verifique a constru√ß√£o da matriz multi-hot.")
        return models
    
    if len(classes_with_samples) <= 10:
        print("   ‚Ä¢ Classes ativas:")
        for idx, count, label in classes_with_samples:
            print(f"      - [{idx}] {label}: {count:,} exemplos")
    else:
        print(f"   ‚Ä¢ Primeiras 5 classes ativas:")
        for idx, count, label in classes_with_samples[:5]:
            print(f"      - [{idx}] {label}: {count:,} exemplos")
        print(f"   ‚Ä¢ ... e mais {len(classes_with_samples) - 5} classes")

    print("\nüöÄ Iniciando treinamento One-vs-Rest...")
    for class_idx in range(n_classes):
        if class_idx % 10 == 0:
            print(f"   ‚Üí Classe {class_idx + 1}/{n_classes} ({all_labels[class_idx]})")

        # Converte para array numpy para garantir opera√ß√µes corretas
        y_class_train = np.asarray(y_train[:, class_idx], dtype=np.float32)
        y_class_val = np.asarray(y_val[:, class_idx], dtype=np.float32)
        
        positive_count = int(y_class_train.sum())
        if positive_count == 0:
            models.append(None)
            continue

        model = CatBoostClassifier(
            iterations=iterations,
            depth=depth,
            learning_rate=learning_rate,
            loss_function="Logloss",
            eval_metric="AUC",
            random_seed=42,
            verbose=False,
            cat_features=cat_feature_indices,
            task_type="CPU",
        )

        # Converte X para array se for memmap (CatBoost pode ter problemas com memmaps)
        X_train_array = np.asarray(X_train) if isinstance(X_train, np.memmap) else X_train
        X_val_array = np.asarray(X_val) if isinstance(X_val, np.memmap) else X_val

        model.fit(
            X_train_array,
            y_class_train,
            eval_set=(X_val_array, y_class_val),
            early_stopping_rounds=50,
            verbose=False,
        )
        models.append(model)

    trained_count = sum(m is not None for m in models)
    print(f"‚úÖ {trained_count} modelos treinados de {n_classes} classes")
    return models


def evaluate(models: list[CatBoostClassifier | None], X_val: np.ndarray, y_val: np.ndarray):
    """Calcula m√©tricas multi-label."""
    if not any(m is not None for m in models):
        print("\n‚ö†Ô∏è Nenhum modelo foi treinado. Pulando avalia√ß√£o.")
        return
    
    n_classes = y_val.shape[1]
    y_pred_proba = np.zeros((X_val.shape[0], n_classes), dtype=np.float32)

    # Converte X_val para array se for memmap
    X_val_array = np.asarray(X_val) if isinstance(X_val, np.memmap) else X_val
    
    for class_idx, model in enumerate(models):
        if model is not None:
            y_pred_proba[:, class_idx] = model.predict_proba(X_val_array)[:, 1]

    # Converte y_val para array numpy (sklearn precisa de array, n√£o memmap)
    y_val_array = np.asarray(y_val, dtype=np.int32)
    y_pred = (y_pred_proba >= 0.5).astype(np.int32)
    
    # Garante que s√£o arrays 2D com formato correto
    if y_val_array.ndim == 1:
        y_val_array = y_val_array.reshape(-1, 1)
    if y_pred.ndim == 1:
        y_pred = y_pred.reshape(-1, 1)
    
    hamming = hamming_loss(y_val_array, y_pred)
    subset_accuracy = accuracy_score(y_val_array, y_pred)

    print("\nüìä Avalia√ß√£o:")
    print(f"   ‚Ä¢ Hamming Loss: {hamming:.4f} (menor √© melhor)")
    print(f"   ‚Ä¢ Subset Accuracy: {subset_accuracy:.4f} (maior √© melhor)")


def save_models(models: list[CatBoostClassifier | None], all_labels: list[str], label_to_idx: dict[str, int], cat_feature_indices: list[int]):
    if not any(m is not None for m in models):
        print("\n‚ö†Ô∏è Nenhum modelo para salvar.")
        return
    
    model_dir = Path("catboost_models")
    model_dir.mkdir(exist_ok=True)

    saved_count = 0
    for class_idx, model in enumerate(models):
        if model is None:
            continue
        safe_label = all_labels[class_idx].replace(" ", "_").replace("/", "_")
        model_path = model_dir / f"catboost_class_{class_idx}_{safe_label}.cbm"
        model.save_model(str(model_path))
        saved_count += 1

    label_info = {
        "all_labels": all_labels,
        "label_to_idx": label_to_idx,
        "n_classes": len(all_labels),
        "cat_feature_indices": cat_feature_indices,
    }
    with open(model_dir / "label_info.pkl", "wb") as f:
        pickle.dump(label_info, f)

    print(f"üíæ {saved_count} modelos e metadados salvos em {model_dir.resolve()}")


# =================================================================
# MAIN
# =================================================================

def main():
    parser = argparse.ArgumentParser(description="Treinamento CatBoost com baixo uso de RAM.")
    parser.add_argument("--val_split", type=float, default=0.2, help="Propor√ß√£o para valida√ß√£o (cont√≠gua).")
    parser.add_argument("--rebuild-cache", action="store_true", help="Recalcula caches de features/labels.")
    parser.add_argument("--iterations", type=int, default=500)
    parser.add_argument("--depth", type=int, default=6)
    parser.add_argument("--learning-rate", type=float, default=0.1)
    args = parser.parse_args()

    print("üìÇ Carregando labels...")
    
    # Diagn√≥stico: verifica o arquivo antes de carregar
    print(f"   ‚Ä¢ Arquivo: {Y_PATH}")
    if not Y_PATH.exists():
        raise FileNotFoundError(f"Arquivo de labels n√£o encontrado: {Y_PATH}")
    
    # L√™ algumas linhas para diagn√≥stico
    with open(Y_PATH, 'r', encoding='utf-8') as f:
        first_lines = [f.readline().strip() for _ in range(5)]
    print(f"   ‚Ä¢ Primeiras 5 linhas do arquivo:")
    for i, line in enumerate(first_lines):
        print(f"      [{i}] {repr(line)[:150]}")
    
    # Carrega o CSV
    y_df = pd.read_csv(Y_PATH)
    print(f"‚úÖ Labels carregados: {len(y_df)} amostras")
    print(f"   ‚Ä¢ Colunas no DataFrame: {list(y_df.columns)}")
    
    # Verifica se a coluna 'behavior' existe
    if 'behavior' not in y_df.columns:
        print(f"   ‚ö†Ô∏è Coluna 'behavior' n√£o encontrada!")
        print(f"   ‚Ä¢ Colunas dispon√≠veis: {list(y_df.columns)}")
        # Tenta usar a primeira coluna
        if len(y_df.columns) > 0:
            first_col = y_df.columns[0]
            print(f"   ‚Ä¢ Usando primeira coluna: '{first_col}'")
            y_df = y_df.rename(columns={first_col: 'behavior'})
        else:
            raise ValueError("DataFrame de labels est√° vazio!")
    
    # Mostra estat√≠sticas dos labels
    non_empty = y_df['behavior'].notna().sum()
    empty = y_df['behavior'].isna().sum()
    print(f"   ‚Ä¢ Registros n√£o vazios: {non_empty:,}")
    print(f"   ‚Ä¢ Registros vazios/NaN: {empty:,}")

    all_labels, label_to_idx = load_label_space(y_df)
    print(f"‚úÖ {len(all_labels)} comportamentos √∫nicos")

    with open(CATEGORICAL_INFO_PATH, "rb") as f:
        categorical_info = pickle.load(f)

    y_multi_hot = ensure_multi_hot_cache(y_df, label_to_idx, rebuild=args.rebuild_cache)
    X_combined, cat_feature_indices = ensure_feature_cache(args.rebuild_cache, categorical_info)

    # Diagn√≥stico da matriz multi-hot
    print("\nüîç Verificando matriz multi-hot...")
    y_multi_hot_array = np.asarray(y_multi_hot)
    total_positives = y_multi_hot_array.sum()
    samples_with_labels = (y_multi_hot_array.sum(axis=1) > 0).sum()
    print(f"   ‚Ä¢ Total de labels ativos: {int(total_positives):,}")
    print(f"   ‚Ä¢ Amostras com pelo menos 1 label: {samples_with_labels:,} de {len(y_multi_hot_array):,}")
    print(f"   ‚Ä¢ M√©dia de labels por amostra: {total_positives / len(y_multi_hot_array):.2f}")
    
    if total_positives == 0:
        raise ValueError("‚ùå A matriz multi-hot est√° vazia! Nenhum label foi encontrado. Verifique o arquivo de labels.")

    total_samples = X_combined.shape[0]
    split_idx = int((1.0 - args.val_split) * total_samples)
    if split_idx == 0 or split_idx == total_samples:
        raise ValueError("Divis√£o treino/val inv√°lida. Ajuste --val_split.")

    X_train = X_combined[:split_idx]
    X_val = X_combined[split_idx:]
    y_train = y_multi_hot[:split_idx]
    y_val = y_multi_hot[split_idx:]

    print(f"\nüìä Divis√£o dos dados (sem c√≥pia):")
    print(f"   ‚Ä¢ Treino: {X_train.shape[0]:,} amostras")
    print(f"   ‚Ä¢ Valida√ß√£o: {X_val.shape[0]:,} amostras")

    models = train_models(
        X_train,
        X_val,
        y_train,
        y_val,
        cat_feature_indices,
        all_labels,
        iterations=args.iterations,
        depth=args.depth,
        learning_rate=args.learning_rate,
    )

    evaluate(models, X_val, y_val)
    save_models(models, all_labels, label_to_idx, cat_feature_indices)

    print("\n" + "=" * 60)
    print("‚úÖ TREINAMENTO CONCLU√çDO")
    print("=" * 60)
    print("\nüí° Dicas:")
    print("   ‚Ä¢ Ajuste --iterations/--depth/--learning-rate conforme mem√≥ria dispon√≠vel.")
    print("   ‚Ä¢ Rode com --rebuild-cache se mudar os arquivos consolidados.")
    print("   ‚Ä¢ Para valida√ß√£o mais aleat√≥ria, gere splits no consolidate antes do cache.")


if __name__ == "__main__":
    main()
