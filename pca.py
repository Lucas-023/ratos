import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# --- CONFIGURA√á√ïES ---
X_FILE = "consolidated_X_FE.npy" 
SAMPLE_SIZE = 50000 
VARIANCE_THRESHOLD = 0.95 
MASK_OUTPUT_FILE = "feature_mask_102.npy"

def load_and_scale_data(file_path: str, sample_size: int) -> tuple[np.ndarray, int]:
    """Carrega, amostra e padroniza os dados, removendo features de vari√¢ncia zero."""
    
    print(f"Carregando dados de {file_path}...")
    try:
        X_data_full = np.load(file_path, mmap_mode='r')
    except Exception as e:
        print(f"Erro ao carregar o arquivo {file_path}: {e}")
        return np.array([]), 0

    if X_data_full.shape[0] > sample_size:
        print(f"Amostrando {sample_size} frames para agilizar a PCA...")
        sample_indices = np.random.choice(X_data_full.shape[0], sample_size, replace=False)
        X_data_sample = X_data_full[sample_indices]
    else:
        print("Usando o dataset completo.")
        X_data_sample = X_data_full[:]
    
    original_features_count = X_data_sample.shape[1]
    print(f"Dimens√µes para PCA: {X_data_sample.shape}")
    
    # 2. Remo√ß√£o de Features com Vari√¢ncia Quase Zero
    stds = np.std(X_data_sample, axis=0)
    non_zero_std_indices = stds > 1e-6 
    
    # üö® A√á√ÉO CR√çTICA: SALVANDO A M√ÅSCARA üö®
    np.save(MASK_OUTPUT_FILE, non_zero_std_indices)
    print(f"‚úÖ M√°scara de features salva em {MASK_OUTPUT_FILE}.")
    
    X_clean = X_data_sample[:, non_zero_std_indices]
    
    print(f"Removidas {original_features_count - X_clean.shape[1]} features de vari√¢ncia zero.")
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_clean)
    
    return X_scaled, original_features_count

def run_pca_analysis(scaled_data: np.ndarray, original_features: int):
    # ... (restante do c√≥digo PCA, que funcionou anteriormente) ...
    if scaled_data.size == 0:
        print("N√£o h√° dados v√°lidos para executar o PCA.")
        return

    print("\nExecutando PCA...")
    pca = PCA(n_components=None) 
    pca.fit(scaled_data)
    
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
    
    n_components = np.where(cumulative_variance >= VARIANCE_THRESHOLD)[0][0] + 1
    
    print(f"=======================================================")
    print(f"‚úÖ AN√ÅLISE PCA CONCLU√çDA")
    print(f"=======================================================")
    print(f"N√∫mero total de features √öTEIS: {pca.n_components_}")
    print(f"Para capturar {VARIANCE_THRESHOLD*100}% da vari√¢ncia, s√£o necess√°rios:")
    print(f"‚û°Ô∏è {n_components} Componentes Principais")
    print(f"=======================================================")
    
    # Este c√≥digo plota o gr√°fico, mas n√£o vou inclu√≠-lo para evitar polui√ß√£o
    # plt.figure(figsize=(10, 6)); plt.plot(...); plt.show()


if __name__ == "__main__":
    scaled_data, original_features = load_and_scale_data(X_FILE, SAMPLE_SIZE)
    run_pca_analysis(scaled_data, original_features)